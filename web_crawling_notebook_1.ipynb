{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18392f3",
   "metadata": {},
   "source": [
    "# Web Crawling with Python â€” Notebook 1\n",
    "## Introduction to Web Crawling\n",
    "\n",
    "---\n",
    "\n",
    "### What is Web Crawling?\n",
    "\n",
    "A **web crawler** is a program that automatically navigates the web, downloads web pages, and extracts useful information.  \n",
    "Crawlers are used for search engines, data analysis, research, and more.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 1: Why Web Crawling?\n",
    "\n",
    "List some real-life examples where web crawling could be useful.\n",
    "\n",
    "_Write your ideas below:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b36cb6",
   "metadata": {},
   "source": [
    "## Search engine optimization; \n",
    "- Indexing\n",
    "- Ranking\n",
    "- Discoverability. \n",
    "## Data collection and analysis; \n",
    "- Competitive analysis\n",
    "- Data mining, \n",
    "- Monitoring website health.\n",
    "## Information retrieval and management\n",
    "- Content Aggregation \n",
    "- AI and Machine learning\n",
    "- Real time update "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195d0c00",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Installing Required Libraries\n",
    "\n",
    "For web crawling in Python, you will usually need:\n",
    "- `requests` for downloading web pages\n",
    "- `BeautifulSoup` (from `bs4`) for parsing HTML\n",
    "\n",
    "You can install them using pip.\n",
    "\n",
    "**Starter:**  \n",
    "Uncomment the code below if you need to install the libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fec686",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f306ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d1d78c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: Importing Libraries\n",
    "\n",
    "Import the libraries you will need for the following tasks.\n",
    "\n",
    "**Starter:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe2c852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import requests and BeautifulSoup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e9439",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Downloading a Web Page\n",
    "\n",
    "Set a variable `url` to `\"https://www.example.com\"`  \n",
    "Use `requests.get()` to download it and assign the response to a variable.\n",
    "\n",
    "**Starter:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8593db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    url = \"https://realpython.github.io/fake-jobs/\"\n",
    "    page = requests.get(url)\n",
    "\n",
    "# TODO: Download the page and store the response\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191fcc2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 5: Checking the Response\n",
    "\n",
    "Print the **status code** of the response to see if the request was successful (should be 200).\n",
    "\n",
    "**Starter:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a2500b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   Fake Python\n",
      "  </title>\n",
      "  <link href=\"https://cdn.jsdelivr.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the status code of the response\n",
    "url = \"https://realpython.github.io/fake-jobs/\"\n",
    "soup = get_soup(url)\n",
    "print(soup.prettify()[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a695080",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 6: Viewing the Page Content\n",
    "\n",
    "Print the first 500 characters of the HTML content you downloaded.\n",
    "\n",
    "**Starter:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a502a418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   Fake Python\n",
      "  </title>\n",
      "  <link href=\"https://cdn.jsdelivr.net/npm/bulma@0.9.2/css/bulma.min.css\" rel=\"stylesheet\"/>\n",
      " </head>\n",
      " <body>\n",
      "  <section class=\"section\">\n",
      "   <div class=\"container mb-5\">\n",
      "    <h1 class=\"title is-1\">\n",
      "     Fake Python\n",
      "    </h1>\n",
      "    <p class=\"subtitle is-3\">\n",
      "     Fake Jobs for Your Web Scraping Journey\n",
      "    </p>\n",
      "   </div>\n",
      "   <div class=\"c\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the first 500 characters of the response text\n",
    "soup = get_soup(url)\n",
    "print(soup.prettify()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c4021b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge: Try Another Site\n",
    "\n",
    "Repeat the download process for another website, for example `\"https://www.python.org\"`.  \n",
    "Print its status code and the first 500 characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4de35b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   Fake Python\n",
      "  </title>\n",
      "  <link href=\"https://cdn.jsdelivr.net/npm/bulma@0.9.2/css/bulma.min.css\" rel=\"stylesheet\"/>\n",
      " </head>\n",
      " <body>\n",
      "  <section class=\"section\">\n",
      "   <div class=\"container mb-5\">\n",
      "    <h1 class=\"title is-1\">\n",
      "     Fake Python\n",
      "    </h1>\n",
      "    <p class=\"subtitle is-3\">\n",
      "     Fake Jobs for Your Web Scraping Journey\n",
      "    </p>\n",
      "   </div>\n",
      "   <div class=\"c\n"
     ]
    }
   ],
   "source": [
    "# TODO: Download another site and print its status code and first 500 characters\n",
    "url = \"https://www.python.org\"\n",
    "page = requests.get(url)\n",
    "soup = get_soup(url)\n",
    "print(soup.prettify()[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c620d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection\n",
    "\n",
    "- What kinds of information do you see in the HTML?\n",
    "- Did you encounter any issues or errors?\n",
    "- What do you think you could do with this data in the next steps?\n",
    "\n",
    "_Write your thoughts here:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea1b5d",
   "metadata": {},
   "source": [
    "At the course of html inspection of elements to identify selectors and element dectectable by beautiful soup (prettify()) function I discovered that they are element that are undiscoverable which makes it difficult to wrap around the correct element.\n",
    "I did not encounter any issues or error. However, I was showed some possible errors and how to avoid them.\n",
    "I can clean this data and transform it after clean it to perform specific tasks for specific result. \n",
    "For example applying for a specific job with the precise link.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eba6870",
   "metadata": {},
   "source": [
    "# Web Crawling with Python â€” Project\n",
    "## Crawling and Analyzing Job Postings\n",
    "\n",
    "---\n",
    "\n",
    "### Context\n",
    "\n",
    "Job listings are a valuable source of data for understanding labor market trends, popular skills, and opportunities by location.\n",
    "\n",
    "**Ethics Notice:**  \n",
    "Never scrape real sites without permission or violating their terms of service. For this project, use a public demo site (e.g. https://realpython.github.io/fake-jobs/), a local HTML file, or a permitted sandbox.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b2ea0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Planning\n",
    "\n",
    "Visit https://realpython.github.io/fake-jobs/ and inspect several job postings.  \n",
    "Sketch a plan for the main steps of your crawler:\n",
    "\n",
    "- Download HTML\n",
    "- Parse the HTML\n",
    "- Extract the relevant fields (title, company, location, description, link)\n",
    "- Store data in a structured format\n",
    "- Analyze and visualize the data\n",
    "\n",
    "_List your plan or make a diagram below:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ea15aa",
   "metadata": {},
   "source": [
    "### TODO: List your plan here or draw a diagram (you can use Markdown cells for diagrams!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04eb6a",
   "metadata": {},
   "source": [
    "# Import the libraries and dependencies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ebcfb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Download and Parse the Webpage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f77421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    \"\"\"\n",
    "    Download the page content from `url` and return a BeautifulSoup object.\n",
    "    HINT: Use requests and BeautifulSoup.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    \n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    return soup \n",
    "\n",
    "# Test: Try calling get_soup with the jobs site URL\n",
    "job_url = \"https://realpython.github.io/fake-jobs/\"\n",
    "soup = get_soup(job_url)\n",
    "print(soup.prettify()[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382ebb9a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: Extract Job Postings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_jobs(soup):\n",
    "    \"\"\"\n",
    "    Find and extract all job postings from the BeautifulSoup object.\n",
    "    Each job should include: title, company, location, description, and link.\n",
    "    HINT: Identify the main container for each job posting. Use soup.find_all and element.find.\n",
    "    Return a list of job dictionaries.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    job_card = results.find_all(\"div\", class_=\"card-content\")\n",
    "    \n",
    "    for job_card in job_cards:\n",
    "        title_element = job_card.find(\"h2\", class_=\"title\")\n",
    "        company_element = job_card.find(\"h3\", class_=\"company\")\n",
    "        location_element = job_card.find(\"p\", class_=\"location\")\n",
    "        ink_element = job_card.find(\"a\", class_=\"href\")\n",
    "        jobs.append({\n",
    "             \"title\": title,\n",
    "             \"company\": company,\n",
    "             \"location\": location,\n",
    "             \"link\": link\n",
    "         })\n",
    "    return jobs\n",
    "\n",
    "# HINT: Print the length of the jobs list and show a sample job.\n",
    "print(len(jobs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f139157",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ff96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_job_data(jobs):\n",
    "    \"\"\"\n",
    "    Clean and normalize job data if necessary.\n",
    "    HINT: Remove extra whitespace, handle missing fields, standardize case, etc.\n",
    "    Return a new list of cleaned job dicts.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    for job_card in job_cards:\n",
    "        title_element = job_card.find(\"h2\", class_=\"title\")\n",
    "        company_element = job_card.find(\"h3\", class_=\"company\")\n",
    "        location_element = job_card.find(\"p\", class_=\"location\")\n",
    "        print(title_element.text.strip())\n",
    "        print(company_element.text.strip())\n",
    "        print(location_element.text.strip())\n",
    "        \n",
    "    pass\n",
    "python_jobs = results.find_all(\n",
    "    \"h2\", string=lambda text: \"python\" in text.lower()\n",
    ")\n",
    "\n",
    "# HINT: What fields might need cleaning in this dataset?\n",
    "python_jobs = results.find_all(\"h2\", string=\"Python\")\n",
    "print (len(python_jobs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ad1f0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 5: Filtering and Searching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_jobs_by_keyword(jobs, keyword):\n",
    "    \"\"\"\n",
    "    Return all jobs where the keyword appears in the title or description (case-insensitive).\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "    python_jobs = results.find_all(\n",
    "    \"h2\", string=lambda text: \"python\" in text.lower()\n",
    "    )\n",
    "\n",
    "    python_job_cards = [\n",
    "        h2_element.parent.parent.parent for h2_element in python_jobs\n",
    "    ]\n",
    "    \n",
    "    \n",
    "\n",
    "def count_jobs_by_field(jobs, field):\n",
    "    \"\"\"\n",
    "    Count jobs by a specific field, e.g., 'location' or 'company'.\n",
    "    Return a dict {field_value: count}\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    \n",
    "    pass \n",
    "    for job_card in python_job_cards:\n",
    "        title_element = job_card.find(\"h2\", class_=\"title\")\n",
    "        company_element = job_card.find(\"h3\", class_=\"company\")\n",
    "        location_element = job_card.find(\"p\", class_=\"location\")\n",
    "        print(title_element.text.strip())\n",
    "        print(company_element.text.strip())\n",
    "        print(location_element.text.strip())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7039fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 6: Analysis and Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6207f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_jobs_by_location(job_counts):\n",
    "    \"\"\"\n",
    "    Plot a bar chart of the number of jobs per location.\n",
    "    HINT: Use matplotlib.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "def plot_top_companies(job_counts, top_n=5):\n",
    "    \"\"\"\n",
    "    Plot a pie chart of the top N companies by number of job postings.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea076e65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Pagination\n",
    "\n",
    "The example site does not have pagination, but many real job boards do.\n",
    "How would you modify your crawler to follow \"next page\" links and collect jobs from multiple pages?\n",
    "_Describe your approach here:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b5bd7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22e42deb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Exporting Data\n",
    "\n",
    "How would you save your job data to a CSV or JSON file?\n",
    "_Outline your steps or code below:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a674fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f10fb3fe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection\n",
    "\n",
    "- Did you notice any anti-scraping protections?\n",
    "- How can you make your crawler more polite (e.g., delays, headers, respecting robots.txt)?\n",
    "- Why is it important to use demo/test data when learning scraping?\n",
    "\n",
    "_Write your thoughts below:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a1547",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

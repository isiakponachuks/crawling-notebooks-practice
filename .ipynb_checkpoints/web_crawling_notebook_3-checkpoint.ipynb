{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "280d638b",
   "metadata": {},
   "source": [
    "# Web Crawling with Python — Notebook 3\n",
    "## Crawling Multiple Pages\n",
    "\n",
    "---\n",
    "\n",
    "### Why Crawl Multiple Pages?\n",
    "\n",
    "Many websites organize content across multiple pages (pagination).  \n",
    "A crawler should be able to navigate through these pages and collect data from all of them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04456ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Review — Download and Parse a Single Page\n",
    "\n",
    "Below is a pattern you’ve seen before:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180dca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://quotes.toscrape.com/page/1/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# TODO: Print the page title (as a warmup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d68d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Find and Print All Quotes on the Page\n",
    "\n",
    "This website has quotes inside `<span class=\"text\">`.\n",
    "\n",
    "**Your task:**  \n",
    "- Find all `<span>` tags with class `\"text\"`\n",
    "- Print the text of each quote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30978f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter:\n",
    "quotes = soup.find_all(\"span\", class_=\"text\")\n",
    "\n",
    "# TODO: Loop through 'quotes' and print each quote text\n",
    "for quote in quotes:\n",
    "     print(quote, end=\"\\n\" * 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e8dbd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: Crawling Multiple Pages (Pagination)\n",
    "\n",
    "On this site, there is a **Next** button at the bottom of each page.  \n",
    "The button’s HTML looks like: `<li class=\"next\"><a href=\"/page/2/\">Next <span aria-hidden=\"true\">→</span></a></li>`\n",
    "\n",
    "**Your task:**  \n",
    "- Check if there is a **Next** page\n",
    "- If so, get its URL and repeat the crawling process\n",
    "\n",
    "**Hint:** Use a `while` loop.\n",
    "\n",
    "**Below is a starter:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58efe38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://quotes.toscrape.com\"\n",
    "page_url = \"/page/1/\"\n",
    "all_quotes = []\n",
    "\n",
    "while page_url:\n",
    "    full_url = base_url + page_url\n",
    "    response = requests.get(full_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # TODO: Extract and print quotes on this page (reuse your code from above)\n",
    "    quotes = soup.find_all(\"div\", class_=\"quote\")\n",
    "    for quote in quotes:\n",
    "         print(f\"Quote: {text}\")\n",
    "    \n",
    "\n",
    "    # Find the Next button ad update page_url (or set to None if there is no next page)\n",
    "    next_btn = soup.find(\"li\", class_=\"next\")\n",
    "    if next_btn:\n",
    "        next_link = next_btn.find(\"a\")[\"href\"]\n",
    "        page_url = next_link\n",
    "    else:\n",
    "        page_url = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab2a59",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Store All Quotes\n",
    "\n",
    "**Modify your loop:**  \n",
    "Instead of just printing, **append** all quote texts to the `all_quotes` list.\n",
    "\n",
    "After crawling all pages, print the total number of quotes collected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e22da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Store all quotes in the all_quotes list, then print the total count at the end\n",
    "all_quotes.append({\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "print(all_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7e6cc5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge: Save Quotes to a File\n",
    "\n",
    "**Optional:**  \n",
    "- Write all the quotes you collected into a text file, one per line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb3d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save the quotes in 'all_quotes' to a file (e.g., quotes.txt)\n",
    "print(all_quotes.txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6471e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection\n",
    "\n",
    "- What challenges did you face with pagination?\n",
    "- How might you adapt this approach for sites with more complex navigation?\n",
    "\n",
    "- Write your thoughts here:_\n",
    "\n",
    "- The primary challenges with the specific pagination approach shown (looking for a li with class next and an \"a\" tag within it) stem from the variability of website structures Inconsistent HTML Structure, Rate Limiting and IP Blocking, Session/Login Requirements\n",
    "- In summary, the key to adapting for complex pagination is a deeper understanding of how the target website works, particularly how it loads new content. This almost always involves careful inspection of the HTML structure and often, the network requests made by your browser."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

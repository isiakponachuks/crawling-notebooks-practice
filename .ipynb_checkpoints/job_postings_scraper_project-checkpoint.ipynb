{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eba6870",
   "metadata": {},
   "source": [
    "# Web Crawling with Python â€” Project\n",
    "## Crawling and Analyzing Job Postings\n",
    "\n",
    "---\n",
    "\n",
    "### Context\n",
    "\n",
    "Job listings are a valuable source of data for understanding labor market trends, popular skills, and opportunities by location.\n",
    "\n",
    "**Ethics Notice:**  \n",
    "Never scrape real sites without permission or violating their terms of service. For this project, use a public demo site (e.g. https://realpython.github.io/fake-jobs/), a local HTML file, or a permitted sandbox.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b2ea0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Planning\n",
    "\n",
    "Visit https://realpython.github.io/fake-jobs/ and inspect several job postings.  \n",
    "Sketch a plan for the main steps of your crawler:\n",
    "\n",
    "- Download HTML\n",
    "- Parse the HTML\n",
    "- Extract the relevant fields (title, company, location, description, link)\n",
    "- Store data in a structured format\n",
    "- Analyze and visualize the data\n",
    "\n",
    "_List your plan or make a diagram below:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ea15aa",
   "metadata": {},
   "source": [
    "### TODO: List your plan here or draw a diagram (you can use Markdown cells for diagrams!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04eb6a",
   "metadata": {},
   "source": [
    "# Import the libraries and dependencies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ebcfb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Download and Parse the Webpage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f77421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    \"\"\"\n",
    "    Download the page content from `url` and return a BeautifulSoup object.\n",
    "    HINT: Use requests and BeautifulSoup.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    \n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    return soup \n",
    "\n",
    "# Test: Try calling get_soup with the jobs site URL\n",
    "job_url = \"https://realpython.github.io/fake-jobs/\"\n",
    "soup = get_soup(job_url)\n",
    "print(soup.prettify()[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382ebb9a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: Extract Job Postings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_jobs(soup):\n",
    "    \"\"\"\n",
    "    Find and extract all job postings from the BeautifulSoup object.\n",
    "    Each job should include: title, company, location, description, and link.\n",
    "    HINT: Identify the main container for each job posting. Use soup.find_all and element.find.\n",
    "    Return a list of job dictionaries.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    job_card = results.find_all(\"div\", class_=\"card-content\")\n",
    "    \n",
    "    for job_card in job_cards:\n",
    "        title_element = job_card.find(\"h2\", class_=\"title\")\n",
    "        company_element = job_card.find(\"h3\", class_=\"company\")\n",
    "        location_element = job_card.find(\"p\", class_=\"location\")\n",
    "        ink_element = job_card.find(\"a\", class_=\"href\")\n",
    "        jobs.append({\n",
    "             \"title\": title,\n",
    "             \"company\": company,\n",
    "             \"location\": location,\n",
    "             \"link\": link\n",
    "         })\n",
    "    return jobs\n",
    "\n",
    "# HINT: Print the length of the jobs list and show a sample job.\n",
    "print(len(jobs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f139157",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ff96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_job_data(jobs):\n",
    "    \"\"\"\n",
    "    Clean and normalize job data if necessary.\n",
    "    HINT: Remove extra whitespace, handle missing fields, standardize case, etc.\n",
    "    Return a new list of cleaned job dicts.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    for job_card in job_cards:\n",
    "        title_element = job_card.find(\"h2\", class_=\"title\")\n",
    "        company_element = job_card.find(\"h3\", class_=\"company\")\n",
    "        location_element = job_card.find(\"p\", class_=\"location\")\n",
    "        print(title_element.text.strip())\n",
    "        print(company_element.text.strip())\n",
    "        print(location_element.text.strip())\n",
    "        \n",
    "    pass\n",
    "python_jobs = results.find_all(\n",
    "    \"h2\", string=lambda text: \"python\" in text.lower()\n",
    ")\n",
    "\n",
    "# HINT: What fields might need cleaning in this dataset?\n",
    "python_jobs = results.find_all(\"h2\", string=\"Python\")\n",
    "print (len(python_jobs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ad1f0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 5: Filtering and Searching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_jobs_by_keyword(jobs, keyword):\n",
    "    \"\"\"\n",
    "    Return all jobs where the keyword appears in the title or description (case-insensitive).\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "    python_jobs = results.find_all(\n",
    "    \"h2\", string=lambda text: \"python\" in text.lower()\n",
    "    )\n",
    "\n",
    "    python_job_cards = [\n",
    "        h2_element.parent.parent.parent for h2_element in python_jobs\n",
    "    ]\n",
    "    \n",
    "    \n",
    "\n",
    "def count_jobs_by_field(jobs, field):\n",
    "    \"\"\"\n",
    "    Count jobs by a specific field, e.g., 'location' or 'company'.\n",
    "    Return a dict {field_value: count}\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    \n",
    "    pass \n",
    "    for job_card in python_job_cards:\n",
    "        title_element = job_card.find(\"h2\", class_=\"title\")\n",
    "        company_element = job_card.find(\"h3\", class_=\"company\")\n",
    "        location_element = job_card.find(\"p\", class_=\"location\")\n",
    "        print(title_element.text.strip())\n",
    "        print(company_element.text.strip())\n",
    "        print(location_element.text.strip())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7039fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 6: Analysis and Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6207f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_jobs_by_location(job_counts):\n",
    "    \"\"\"\n",
    "    Plot a bar chart of the number of jobs per location.\n",
    "    HINT: Use matplotlib.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "    if not job_counts:\n",
    "        print(\"No job location data to plot.\")\n",
    "        return\n",
    "\n",
    "    locations = list(job_counts.keys())\n",
    "    counts = list(job_counts.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6)) # Adjust figure size for better readability\n",
    "    plt.bar(locations, counts, color='skyblue')\n",
    "    plt.xlabel(\"Location\")\n",
    "    plt.ylabel(\"Number of Job Postings\")\n",
    "    plt.title(\"Number of Job Postings by Location\")\n",
    "    plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for better readability\n",
    "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "    plt.show()\n",
    "\n",
    "def plot_top_companies(job_counts, top_n=5):\n",
    "    \"\"\"\n",
    "    Plot a pie chart of the top N companies by number of job postings.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "    if not job_counts:\n",
    "        print(\"No data to plot.\")\n",
    "        return\n",
    "\n",
    "    # Sort companies by job count in descending order\n",
    "    sorted_companies = sorted(job_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Get the top N companies\n",
    "    top_n_companies = sorted_companies[:top_n]\n",
    "    \n",
    "    # Separate names and counts for plotting\n",
    "    company_names = [company[0] for company in top_n_companies]\n",
    "    job_counts_values = [company[1] for company in top_n_companies]\n",
    "\n",
    "    # Handle \"Other\" category if there are more than top_n companies\n",
    "    if len(sorted_companies) > top_n:\n",
    "        other_count = sum(company[1] for company in sorted_companies[top_n:])\n",
    "        company_names.append(f'Other ({len(sorted_companies) - top_n} companies)')\n",
    "        job_counts_values.append(other_count)\n",
    "\n",
    "    plt.figure(figsize=(8, 8)) # Make the figure square for a better looking pie chart\n",
    "    plt.pie(job_counts_values, labels=company_names, autopct='%1.1f%%', startangle=140, pctdistance=0.85)\n",
    "    plt.title(f\"Top {top_n} Companies by Job Postings\")\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea076e65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Pagination\n",
    "\n",
    "The example site does not have pagination, but many real job boards do.\n",
    "How would you modify your crawler to follow \"next page\" links and collect jobs from multiple pages?\n",
    "_Describe your approach here:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b5bd7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c49493d",
   "metadata": {},
   "source": [
    "This often involves inspecting the HTML structure of the page, locating the \"next page\" link (which could be text, an image, or a button), and extracting its href attribute (or the equivalent if it's not an anchor tag). Then, the crawler needs to be instructed to add this new URL to the queue of URLs to be processed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e42deb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Exporting Data\n",
    "\n",
    "How would you save your job data to a CSV or JSON file?\n",
    "_Outline your steps or code below:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a674fe",
   "metadata": {},
   "source": [
    "The python libraries contain capabilities that can tranform data into json or CSV files, for a json file which looks like a dictionary, we need to first import a json library  \n",
    "import json\n",
    "\n",
    "### extract the data into an array\n",
    "extracted_data = []\n",
    "\n",
    "### append the array file for collection\n",
    "extract_data.append(data)\n",
    "\n",
    "### print the entire json file that is now clean to be saved (\"name.json\") \n",
    "print(json.dumps(extracted_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10fb3fe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection\n",
    "\n",
    "- Did you notice any anti-scraping protections?\n",
    "- How can you make your crawler more polite (e.g., delays, headers, respecting robots.txt)?\n",
    "- Why is it important to use demo/test data when learning scraping?\n",
    "\n",
    "_Write your thoughts below:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a1547",
   "metadata": {},
   "source": [
    "No, I not did notice any anti-scraping protection. However, it is important to be careful implementing crawling to avoid IP blocking, user agent detection, and more.\n",
    "The robots.txt file specifies which parts of a website crawlers are allowed to access. Always check this file before crawling and adhere to its rules. \n",
    "Introduce delays between requests to avoid overwhelming the target website's server. This can be a fixed delay or a randomized delay. \n",
    "Websites analyze request headers to identify bots. Mimic real browser behavior by using realistic User-Agent strings, Referer headers, and other relevant headers. \n",
    "If scraping a large number of pages, consider using IP rotation to distribute requests across multiple IP addresses, making it harder to detect and block your crawler. \n",
    "Similar to IP rotation, rotating User-Agent strings can help avoid detection based on user agent analysis. \n",
    "Why Use Demo/Test Data?\n",
    "Scraping real websites without caution can lead to your IP address being blocked, especially if you're sending many requests or violating their terms of service. Using demo data prevents this.\n",
    "Testing your scraper on demo data allows you to experiment with different approaches and refine your code without the risk of disrupting real website operations.\n",
    "Demo data can help you understand the structure of the website you want to scrape, allowing you to develop more robust and efficient scraping logic.\n",
    "Real websites may have dynamic elements, changing structures, or other unexpected behaviors. Testing on demo data allows you to develop comprehensive error handling routines. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

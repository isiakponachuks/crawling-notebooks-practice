{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3448a5e",
   "metadata": {},
   "source": [
    "# Web Crawling with Python — Notebook 4\n",
    "## Handling Common Crawling Issues\n",
    "\n",
    "---\n",
    "\n",
    "### Why Handle Issues?\n",
    "\n",
    "Crawling is not always smooth!  \n",
    "Websites may block requests, move content, or show errors.  \n",
    "You must make your crawler robust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e3f0fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Handling Request Errors\n",
    "\n",
    "When making requests, things can go wrong:\n",
    "- Page not found (404)\n",
    "- Server error (500)\n",
    "- Connection timeouts\n",
    "\n",
    "**Starter:**  \n",
    "The code below tries to download a page and checks for a successful response.\n",
    "\n",
    "**Your task:**  \n",
    "- Use a try/except block to catch exceptions.\n",
    "- Print a message if the request fails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bfee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://quotes.toscrape.com/nonexistent-page\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, timeout=5)\n",
    "    # TODO: Check if status code is 200. If not, print the code and a message.\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Received status code {response.status_code} for URL: {url}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Successfully fetched {url} with status code {response.status_code}.\")\n",
    "        \n",
    "# You can add further processing of response.text here if needed\n",
    "except requests.exceptions.Timeout:\n",
    "    print(f\"Error: Request to {url} timed out after 5 seconds.\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(f\"Error: Could not connect to {url}. Check your internet connection or URL.\")\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"Error: HTTP error occurred for {url} - {e}\")\n",
    "except Exception as e:\n",
    "\n",
    "    # TODO: Print an error message and the exception\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d636c2a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Handling Delays (Be Polite!)\n",
    "\n",
    "If you crawl too quickly, you may overload servers or get blocked.  \n",
    "A polite crawler **waits** between requests.\n",
    "\n",
    "**Your task:**  \n",
    "- Use `time.sleep()` to pause for 1 second after each request in a loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac8700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Example of crawling multiple pages (simulate)\n",
    "urls = [\n",
    "    \"https://quotes.toscrape.com/page/1/\",\n",
    "    \"https://quotes.toscrape.com/page/2/\"\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    print(f\"Crawled {url}\")\n",
    "    # TODO: Pause for 1 second here before the next request\n",
    "    print(\"pausing for a second.......\")\n",
    "    \n",
    "print(\"Finished crawling all URLs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2d2e6d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: Detecting and Respecting robots.txt\n",
    "\n",
    "Most websites have a `robots.txt` file describing rules for crawlers.  \n",
    "While Python has a module for this, let’s first **fetch and print** the robots.txt contents.\n",
    "\n",
    "**Your task:**  \n",
    "- Download and print the contents of `robots.txt` for a site (e.g., \"https://quotes.toscrape.com/robots.txt\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "robots_url = \"https://quotes.toscrape.com/robots.txt\"\n",
    "# TODO: Download and print the robots.txt content\n",
    "print(f\"An unexpected error occurred while fetching robots.txt: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5a66e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Faking User-Agent Headers\n",
    "\n",
    "Some websites block requests with a default Python user-agent.  \n",
    "You can fake a browser header to appear more like a real visitor.\n",
    "\n",
    "**Starter:**  \n",
    "A common way to set headers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b31962",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://quotes.toscrape.com\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "    # You can try changing this string!\n",
    "}\n",
    "# TODO: Make a request with headers and print the status code\n",
    "try:\n",
    "    response = requests.get(url, headers=headers, timeout=10) # Added timeout for robustness\n",
    "    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "    print(f\"Request to {url} made with custom headers.\")\n",
    "    \n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"Headers used: {headers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71cd0ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge: Handling Redirects\n",
    "\n",
    "Some pages may redirect your crawler (status codes 301 or 302).\n",
    "\n",
    "**Your task:**  \n",
    "- Use `allow_redirects=False` in `requests.get()`\n",
    "- Print the status code and the value of the `Location` header (if any)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b0e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://quotes.toscrape.com/redirect\"\n",
    "# TODO: Make the request with allow_redirects=False and print results\n",
    "try:\n",
    "    response = requests.get(url, allow_redirects=False, timeout=5)\n",
    "\n",
    "    print(f\"Request URL: {url}\")\n",
    "    \n",
    "    \n",
    "    if 'Location' in response.headers:\n",
    "        print(f\"Location Header (Redirect URL): {response.headers['Location']}\")\n",
    "    else:\n",
    "        print(\"Location Header: Not found (No redirect indicated in headers, or not a redirect response).\")\n",
    "\n",
    "    # You can also see the response history (which will be empty here because redirects are not followed)\n",
    "    print(f\"Response History (redirects followed by requests): {response.history}\")\n",
    "\n",
    "except requests.exceptions.Timeout:\n",
    "    print(f\"Error: Request to {url} timed out after 5 seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf77661",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection\n",
    "\n",
    "- Which of these issues did you find most important?  \n",
    "- How could you improve your crawler to be more polite or robust?\n",
    "\n",
    "- Write your thoughts here:_\n",
    "- Being polite is crucial for avoiding IP bans, legal issues, and being a good internet citizen\n",
    "- respecting robots.txt \n",
    "- Robustness ensures your crawler doesn't crash and can handle unexpected situations gracefully, providing more reliable data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
